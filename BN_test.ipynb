{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I need to duplicate the fragility table a bunch of times for the training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to check the combined fragility. Under certain circumstances, it doesn't give sensible results (e.g. Shaking DS1, Tsunami DS2, Landslide DS0, combined is all 20% for some reason???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:06.245034Z",
     "start_time": "2021-06-10T02:18:06.200060Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.215577Z",
     "start_time": "2021-06-10T02:18:06.246033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\Scripts\\jupyter-nbextension-script.py\", line 9, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\jupyter_core\\application.py\", line 269, in launch_instance\n",
      "    return super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\notebook\\nbextensions.py\", line 972, in start\n",
      "    super().start()\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\jupyter_core\\application.py\", line 258, in start\n",
      "    self.subapp.start()\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\notebook\\nbextensions.py\", line 882, in start\n",
      "    self.toggle_nbextension_python(self.extra_args[0])\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\notebook\\nbextensions.py\", line 855, in toggle_nbextension_python\n",
      "    return toggle(module,\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\notebook\\nbextensions.py\", line 470, in enable_nbextension_python\n",
      "    return _set_nbextension_state_python(True, module, user, sys_prefix,\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\notebook\\nbextensions.py\", line 368, in _set_nbextension_state_python\n",
      "    m, nbexts = _get_nbextension_metadata(module)\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\notebook\\nbextensions.py\", line 1107, in _get_nbextension_metadata\n",
      "    m = import_item(module)\n",
      "  File \"C:\\Users\\shannonb\\AppData\\Local\\miniforge3\\envs\\pac_model\\lib\\site-packages\\traitlets\\utils\\importstring.py\", line 38, in import_item\n",
      "    return __import__(parts[0])\n",
      "ModuleNotFoundError: No module named 'ipyleaflet'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BNModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChainMap\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBNModel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BNModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'BNModel'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pysmile\n",
    "import pysmile_license\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('/src/python_classes')\n",
    "import rpy2\n",
    "# os.environ['R_HOME'] = 'C:\\ProgramData\\Anaconda3\\Lib\\R'\n",
    "# %load_ext rpy2.ipython\n",
    "!jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "# from ipyleaflet import *\n",
    "# import ipywidgets as widgets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "# import geojson\n",
    "import folium\n",
    "from colormap import rgb2hex\n",
    "# import rpy2\n",
    "# os.environ['R_HOME'] = '/lib/R'\n",
    "# %load_ext rpy2.ipython\n",
    "from folium.plugins import FloatImage\n",
    "from collections import ChainMap\n",
    "import re\n",
    "\n",
    "# from BNModel import BNModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = pysmile.Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Hazard Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bin_definitions.json\") as f:\n",
    "    bin_defs_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_defs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for combining DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_states_array = np.array((0,1,2,3,4))\n",
    "landslide_states_array = np.array((0,4))\n",
    "\n",
    "fragility_array = np.array(('Shaking','Tsunami','Landslide'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = list(itertools.product(list(itertools.product(damage_states_array,damage_states_array)),landslide_states_array))\n",
    "\n",
    "df_fragilities = pd.DataFrame.from_dict(dict(ChainMap(*[{i:{\n",
    "    'Shaking':x[0][0],\n",
    "    'Tsunami':x[0][1],\n",
    "    'Landslide':x[1]}} for i,x in zip(np.arange(0,len(rows),1),rows)]))).T\n",
    "\n",
    "#### But we can put in percentages.. it doesn't have to be 100% into the one state ####\n",
    "\n",
    "# Rules:\n",
    "# Pick the max damage state. Start with this as the combined damage state.\n",
    "# If there are multiple hazards of the same damage state, add one to the combined damage state\n",
    "# Cap the damage state at 3\n",
    "\n",
    "for index,row in df_fragilities.iterrows():\n",
    "    comb_frag = np.max(row)\n",
    "    # See how many hazards have the max damage state:\n",
    "    number_with_max_damage = list(row).count(comb_frag)\n",
    "#     # If there are more than 2, add one to the combined damage state (unless your at max or no damage)\n",
    "#     if (number_with_max_damage>=2)&(comb_frag!=3)&(comb_frag!=0):\n",
    "#         comb_frag+=1\n",
    "    \n",
    "    # Add to the fragility\n",
    "    df_fragilities.loc[index,'Combined'] = int(comb_frag)\n",
    "    \n",
    "# Rename the columns to include fragility\n",
    "[df_fragilities.rename(columns={x:x+'Fragility'},inplace=True) for x in df_fragilities if x!='Combined']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the integers with strings \n",
    "df_fragilities = df_fragilities.replace(0,'DS0')\n",
    "df_fragilities = df_fragilities.replace(1,'DS1')\n",
    "df_fragilities = df_fragilities.replace(2,'DS2')\n",
    "df_fragilities = df_fragilities.replace(3,'DS3')\n",
    "df_fragilities = df_fragilities.replace(4,'DS4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fragilities.reset_index(drop=True,inplace=True)\n",
    "df_fragilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fragilities_melt = df_fragilities.melt(['ShakingFragility','TsunamiFragility','LandslideFragility'])\n",
    "df_fragilities_melt = df_fragilities_melt.sort_values(['ShakingFragility','TsunamiFragility','LandslideFragility'])\n",
    "df_fragilities_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add_node(pysmile.NodeType.CPT,'CombinedFragility')\n",
    "for outcome in ['DS0','DS1','DS2','DS3','DS4']:\n",
    "    net.add_outcome('CombinedFragility',outcome)\n",
    "\n",
    "net.add_node(pysmile.NodeType.CPT,'ShakingFragility')\n",
    "for outcome in ['DS0','DS1','DS2','DS3','DS4']:\n",
    "    net.add_outcome('ShakingFragility',outcome)\n",
    "net.add_arc('ShakingFragility','CombinedFragility')\n",
    "\n",
    "net.add_node(pysmile.NodeType.CPT,'TsunamiFragility')\n",
    "for outcome in ['DS0','DS1','DS2','DS3','DS4']:\n",
    "    net.add_outcome('TsunamiFragility',outcome)\n",
    "net.add_arc('TsunamiFragility','CombinedFragility')\n",
    "\n",
    "net.add_node(pysmile.NodeType.CPT,'LandslideFragility')\n",
    "for outcome in ['DS0','DS4']:\n",
    "    net.add_outcome('LandslideFragility',outcome)\n",
    "net.add_arc('LandslideFragility','CombinedFragility')\n",
    "\n",
    "\n",
    "for node in ['CombinedFragility','ShakingFragility','TsunamiFragility','LandslideFragility']:\n",
    "    net.delete_outcome(node,'State0')\n",
    "    net.delete_outcome(node,'State1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_fragilities_melt)*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt_list = []\n",
    "for index,row in df_fragilities_melt.iterrows():\n",
    "    if row.value=='DS0':\n",
    "        lst = [1,0,0,0,0]\n",
    "    elif row.value=='DS1':\n",
    "        lst = [0,1,0,0,0]\n",
    "    elif row.value=='DS2':\n",
    "        lst = [0,0,1,0,0]\n",
    "    elif row.value=='DS3':\n",
    "        lst = [0,0,0,1,0]\n",
    "    elif row.value=='DS4':\n",
    "        lst = [0,0,0,0,1]\n",
    "        \n",
    "    cpt_list = cpt_list+lst\n",
    "    \n",
    "net.set_node_definition('CombinedFragility',cpt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fragilities[(df_fragilities.TsunamiFragility=='DS2')&(df_fragilities.LandslideFragility=='DS0')&(df_fragilities.ShakingFragility=='DS1')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pga_general = np.arange(0,5,1)\n",
    "PGA1 = pga_general\n",
    "PGA2 = pga_general\n",
    "PGA3 = pga_general\n",
    "LSA = np.arange(0,3,1)\n",
    "\n",
    "rows = list(itertools.product(list(itertools.product(list(itertools.product(PGA1,PGA2)),PGA3)),LSA))\n",
    "\n",
    "df_pga_cpt = pd.DataFrame.from_dict(dict(ChainMap(*[{i:{\n",
    "    'ShakingFragilityA':x[0][0][0],\n",
    "    'ShakingFragilityB':x[0][0][1],\n",
    "    'ShakingFragilityC':x[0][1],\n",
    "    'LiquefactionFragility':x[1]}} for i,x in zip(np.arange(0,len(rows),1),rows)]))).T\n",
    "    \n",
    "df_pga_cpt.loc[df_pga_cpt.LiquefactionFragility==0,'ShakingFragility'] = df_pga_cpt.loc[df_pga_cpt.LiquefactionFragility==0,'ShakingFragilityA']\n",
    "df_pga_cpt.loc[df_pga_cpt.LiquefactionFragility==1,'ShakingFragility'] = df_pga_cpt.loc[df_pga_cpt.LiquefactionFragility==1,'ShakingFragilityB']\n",
    "df_pga_cpt.loc[df_pga_cpt.LiquefactionFragility==2,'ShakingFragility'] = df_pga_cpt.loc[df_pga_cpt.LiquefactionFragility==2,'ShakingFragilityC']\n",
    "\n",
    "# df_pga_cpt.loc[df_pga_cpt.PGAt==0]\n",
    "# df_pga_cpt.loc[df_pga_cpt.PGAt==1]\n",
    "# df_pga_cpt.loc[df_pga_cpt.PGAt==2]\n",
    "# df_pga_cpt.loc[df_pga_cpt.PGAt==3]\n",
    "# df_pga_cpt.loc[df_pga_cpt.PGAt==4]\n",
    "\n",
    "df_pga_cpt = df_pga_cpt.replace(0,'DS0')\n",
    "df_pga_cpt = df_pga_cpt.replace(1,'DS1')\n",
    "df_pga_cpt = df_pga_cpt.replace(2,'DS2')\n",
    "df_pga_cpt = df_pga_cpt.replace(3,'DS3')\n",
    "df_pga_cpt = df_pga_cpt.replace(4,'DS4')\n",
    "\n",
    "df_pga_cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index,row in df_fragilities.iterrows():\n",
    "#     df = df_pga_cpt[df_pga_cpt.ShakingFragility==row.ShakingFragility]\n",
    "#     df2 = pd.DataFrame([row]*len(df)).merge(df,left_on='ShakingFragility',right_on='ShakingFragility')\n",
    "#     if index!=0:\n",
    "#         df3 = df3.append(df2)\n",
    "#     else:\n",
    "#         df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fragilities = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fragilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fragilities[['ShakingFragilityA','ShakingFragilityB','ShakingFragilityC','LiquefactionFragility','ShakingFragility']].loc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a BN using this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frag_data_dict = {k:np.array(df_fragilities[k]) for k in list(df_fragilities)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# frag_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frag_file_label = 'fragility'\n",
    "\n",
    "# #### Don't include spaces in bin names. if no discretisation, just leave out that key\n",
    "# frag_model_dict = {\n",
    "#     'variables':{\n",
    "#         'ShakingFragility':{\n",
    "#             'label':'ShakingFragility',\n",
    "#             'child_nodes':['Combined'],\n",
    "#             'bins':['DS0','DS1','DS2','DS3','DS4']\n",
    "#         },\n",
    "#         'TsunamiFragility':{\n",
    "#             'label':'TsunamiFragility',\n",
    "#             'child_nodes':['Combined'],\n",
    "#             'bins':['DS0','DS1','DS2','DS3','DS4']\n",
    "#         },\n",
    "#         'LandslideFragility':{\n",
    "#             'label':'LandslideFragility',\n",
    "#             'child_nodes':['Combined'],\n",
    "#             'bins':['DS0','DS4']\n",
    "#         },\n",
    "#         'Combined':{\n",
    "#             'label':'Combined',\n",
    "#             'child_nodes':[],\n",
    "#             'bins':['DS0','DS1','DS2','DS3','DS4']\n",
    "#         }\n",
    "#     },\n",
    "#     'training_frac':0.8,\n",
    "#     'bootstrap_reps':1\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frag_model_dict = BNModel().bootstrap_data(frag_model_dict,frag_data_dict,df_fragilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frag_model_dict = BNModel().discretiser(frag_model_dict,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNModel().save_dataset(frag_model_dict,frag_file_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frag_model_dict = BNModel().create_SM(frag_model_dict,frag_file_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set evidence and get beliefs\n",
    "# frag_model_dict = BNModel().update_evidence(frag_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Additional Shaking Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# frag_model_dict_copy = frag_model_dict.copy()\n",
    "# net = frag_model_dict_copy['model'][0]['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add_node(pysmile.NodeType.CPT,'ShakingFragilityA')\n",
    "for outcome in ['DS0','DS1','DS2','DS3','DS4']:\n",
    "    net.add_outcome('ShakingFragilityA',outcome)\n",
    "net.add_arc('ShakingFragilityA','ShakingFragility')\n",
    "\n",
    "net.add_node(pysmile.NodeType.CPT,'ShakingFragilityB')\n",
    "for outcome in ['DS0','DS1','DS2','DS3','DS4']:\n",
    "    net.add_outcome('ShakingFragilityB',outcome)\n",
    "net.add_arc('ShakingFragilityB','ShakingFragility')\n",
    "\n",
    "net.add_node(pysmile.NodeType.CPT,'ShakingFragilityC')\n",
    "for outcome in ['DS0','DS1','DS2','DS3','DS4']:\n",
    "    net.add_outcome('ShakingFragilityC',outcome)\n",
    "net.add_arc('ShakingFragilityC','ShakingFragility')\n",
    "\n",
    "net.add_node(pysmile.NodeType.CPT,'LiquefactionFragility')\n",
    "for outcome in ['DS0','DS1','DS2']:\n",
    "    net.add_outcome('LiquefactionFragility',outcome)\n",
    "net.add_arc('LiquefactionFragility','ShakingFragility')\n",
    "\n",
    "for node in ['ShakingFragilityA','ShakingFragilityB','ShakingFragilityC','LiquefactionFragility']:\n",
    "    net.delete_outcome(node,'State0')\n",
    "    net.delete_outcome(node,'State1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pga_cpt.set_index('ShakingFragility')#.pivot(columns=['ShakingFragilityA','ShakingFragilityB','ShakingFragilityC','LiquefactionFragility'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pga_cpt_melt = df_pga_cpt.melt(['ShakingFragilityA','ShakingFragilityB','ShakingFragilityC','LiquefactionFragility'])\n",
    "df_pga_cpt_melt = df_pga_cpt_melt.sort_values(['ShakingFragilityA','ShakingFragilityB','ShakingFragilityC','LiquefactionFragility'])\n",
    "\n",
    "cpt_list = []\n",
    "for index,row in df_pga_cpt_melt.iterrows():\n",
    "    if row.value=='DS0':\n",
    "        lst = [1,0,0,0,0]\n",
    "    elif row.value=='DS1':\n",
    "        lst = [0,1,0,0,0]\n",
    "    elif row.value=='DS2':\n",
    "        lst = [0,0,1,0,0]\n",
    "    elif row.value=='DS3':\n",
    "        lst = [0,0,0,1,0]\n",
    "    elif row.value=='DS4':\n",
    "        lst = [0,0,0,0,1]\n",
    "        \n",
    "    cpt_list = cpt_list+lst\n",
    "    \n",
    "net.set_node_definition('ShakingFragility',cpt_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in ['ShakingFragilityA','ShakingFragilityB','ShakingFragilityC']:\n",
    "    net.set_node_definition(node,[1/5]*5)\n",
    "net.set_node_definition('LiquefactionFragility',[1/3]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Delete the premade states\n",
    "# net.delete_outcome(new_node_name,'State0')\n",
    "# net.delete_outcome(new_node_name,'State1')\n",
    "# # Create connection between the hazard and fragility functions\n",
    "\n",
    "# net.add_arc(new_node_name,child_node_name)\n",
    "# print(new_node_name)\n",
    "# print(child_node_name)\n",
    "# # Set the values of the conditional probability table for the fragility node\n",
    "# if new_node_name=='Shaking':\n",
    "#     cpt_value = \\\n",
    "#         list(df_ds_shaking[df_ds_shaking.index==earthquake_low].T[earthquake_low])+\\\n",
    "#         list(df_ds_shaking[df_ds_shaking.index==earthquake_medium].T[earthquake_medium])+\\\n",
    "#         list(df_ds_shaking[df_ds_shaking.index==earthquake_high].T[earthquake_high])\n",
    "# elif new_node_name=='Landslide':\n",
    "#     cpt_value = \\\n",
    "#         list(df_ds_landslide[df_ds_landslide.index==landslide_no].T[landslide_no])+\\\n",
    "#         list(df_ds_landslide[df_ds_landslide.index==landslide_yes].T[landslide_yes])\n",
    "# elif new_node_name=='Tsunami':\n",
    "#     cpt_value = \\\n",
    "#         list(df_ds_tsunami[df_ds_tsunami.index==tsunami_zero].T[tsunami_zero])+\\\n",
    "#         list(df_ds_tsunami[df_ds_tsunami.index==tsunami_low].T[tsunami_low])+\\\n",
    "#         list(df_ds_tsunami[df_ds_tsunami.index==tsunami_medium].T[tsunami_medium])+\\\n",
    "#         list(df_ds_tsunami[df_ds_tsunami.index==tsunami_high].T[tsunami_high])\n",
    "# elif new_node_name=='Liquefaction':\n",
    "#     cpt_value = \\\n",
    "#         list(df_ds_liquefaction[df_ds_liquefaction.index==liquefaction_zero].T[liquefaction_zero])+\\\n",
    "#         list(df_ds_liquefaction[df_ds_liquefaction.index==liquefaction_low].T[liquefaction_low])+\\\n",
    "#         list(df_ds_liquefaction[df_ds_liquefaction.index==liquefaction_medium].T[liquefaction_medium])+\\\n",
    "#         list(df_ds_liquefaction[df_ds_liquefaction.index==liquefaction_high].T[liquefaction_high])\n",
    "# net.set_node_definition(child_node_name,cpt_value)\n",
    "# net.set_node_definition(new_node_name,[1/len(outcomes)]*len(outcomes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating fragility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_bins = {\n",
    "    'low':0.5001, #0-0.25\n",
    "    'medium':0.7001, #0.25-0.75\n",
    "    'high':4.0001 #>.75\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "\n",
    "# Get max mean pga\n",
    "mu_ds1 = 0.0588#bins['minimum']\n",
    "mu_ds2 = 0.0952#2*earthquake_diff+bins['minimum']\n",
    "mu_ds3 = 0.30808#1*earthquake_diff+bins['minimum']\n",
    "mu_ds4 = 0.413616#4*earthquake_diff+bins['minimum']\n",
    "\n",
    "sigma_ds1 = 0.54#0.00000001\n",
    "sigma_ds2 = 0.58#1\n",
    "sigma_ds3 = 0.49#1\n",
    "sigma_ds4 = 0.57#1\n",
    "\n",
    "IM = IM = np.arange(0.0001,10,0.1)#np.arange(bins['minimum'],earthquake_max,(earthquake_max-bins['minimum'])/1000)\n",
    "\n",
    "from scipy.stats import lognorm\n",
    "import math\n",
    "\n",
    "# P_ds1 = [lognorm(s=sigma_ds1).cdf(im)*mu_ds1/lognorm(s=sigma_ds1).mean() for im in IM]\n",
    "# P_ds2 = [lognorm(s=sigma_ds2).cdf(im)*mu_ds2/lognorm(s=sigma_ds2).mean() for im in IM]\n",
    "# P_ds3 = [lognorm(s=sigma_ds3).cdf(im)*mu_ds3/lognorm(s=sigma_ds3).mean() for im in IM]\n",
    "# P_ds4 = [lognorm(s=sigma_ds4).cdf(im)*mu_ds4/lognorm(s=sigma_ds4).mean() for im in IM]\n",
    "\n",
    "P_ds1a = [lognorm(s=sigma_ds1,scale=math.exp(mu_ds1*0.98)).cdf(im) for im in IM]\n",
    "P_ds2a = [lognorm(s=sigma_ds2,scale=math.exp(mu_ds2*0.98)).cdf(im) for im in IM]\n",
    "P_ds3a = [lognorm(s=sigma_ds3,scale=math.exp(mu_ds3*0.98)).cdf(im) for im in IM]\n",
    "P_ds4a = [lognorm(s=sigma_ds4,scale=math.exp(mu_ds4*0.98)).cdf(im) for im in IM]\n",
    "\n",
    "P_ds1b = [lognorm(s=sigma_ds1,scale=math.exp(mu_ds1*.9)).cdf(im) for im in IM]\n",
    "P_ds2b = [lognorm(s=sigma_ds2,scale=math.exp(mu_ds2*.9)).cdf(im) for im in IM]\n",
    "P_ds3b = [lognorm(s=sigma_ds3,scale=math.exp(mu_ds3*.9)).cdf(im) for im in IM]\n",
    "P_ds4b = [lognorm(s=sigma_ds4,scale=math.exp(mu_ds4*.9)).cdf(im) for im in IM]\n",
    "\n",
    "P_ds1c = [lognorm(s=sigma_ds1,scale=math.exp(mu_ds1*.8)).cdf(im) for im in IM]\n",
    "P_ds2c = [lognorm(s=sigma_ds2,scale=math.exp(mu_ds2*.8)).cdf(im) for im in IM]\n",
    "P_ds3c = [lognorm(s=sigma_ds3,scale=math.exp(mu_ds3*.8)).cdf(im) for im in IM]\n",
    "P_ds4c = [lognorm(s=sigma_ds4,scale=math.exp(mu_ds4*.8)).cdf(im) for im in IM]\n",
    "\n",
    "\n",
    "P_ds1d = [lognorm(s=sigma_ds1,scale=math.exp(mu_ds1*.6)).cdf(im) for im in IM]\n",
    "P_ds2d = [lognorm(s=sigma_ds2,scale=math.exp(mu_ds2*.6)).cdf(im) for im in IM]\n",
    "P_ds3d = [lognorm(s=sigma_ds3,scale=math.exp(mu_ds3*.6)).cdf(im) for im in IM]\n",
    "P_ds4d = [lognorm(s=sigma_ds4,scale=math.exp(mu_ds4*.6)).cdf(im) for im in IM]\n",
    "\n",
    "\n",
    "l1 = plt.plot(IM,P_ds1a,c='r')\n",
    "l2 = plt.plot(IM,P_ds2a,c='b')\n",
    "l3 = plt.plot(IM,P_ds3a,c='k')\n",
    "l4 = plt.plot(IM,P_ds4a,c='g')\n",
    "\n",
    "l5 = plt.plot(IM,P_ds1b,ls='--',c='r')\n",
    "l6 = plt.plot(IM,P_ds2b,ls='--',c='b')\n",
    "l7 = plt.plot(IM,P_ds3b,ls='--',c='k')\n",
    "l8 = plt.plot(IM,P_ds4b,ls='--',c='g')\n",
    "\n",
    "l9 = plt.plot(IM,P_ds1c,ls='-.',c='r')\n",
    "l10 = plt.plot(IM,P_ds2c,ls='-.',c='b')\n",
    "l11 = plt.plot(IM,P_ds3c,ls='-.',c='k')\n",
    "l12 = plt.plot(IM,P_ds4c,ls='-.',c='g')\n",
    "\n",
    "# ax.plot([bins['low'],bins['low']],[0,1],ls='--',c='k')\n",
    "# ax.plot([bins['medium'],bins['medium']],[0,1],ls='--',c='k')\n",
    "# ax.plot([bins['high'],bins['high']],[0,1],ls='--',c='k')\n",
    "\n",
    "ax.legend(['DS1','DS2','DS3','DS4'])\n",
    "\n",
    "# plt.plot(IM,P_ds0)\n",
    "# plt.plot(IM,P_ds1)\n",
    "# plt.plot(IM,P_ds2)\n",
    "# plt.plot(IM,P_ds3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mu_ds1 = 0.25\n",
    "# # mu_ds2 = 0.55\n",
    "# # mu_ds3 = 1.28\n",
    "# # mu_ds4 = 2.01\n",
    "\n",
    "# # # Shaking\n",
    "# # mu_list = [0.25,0.55,1.25,2.01]\n",
    "# # sigma_list = [0.64,0.64,0.64,0.64]\n",
    "# ds_list = ['DS0','DS1','DS2','DS3','DS4']\n",
    "# IM = np.arange(0.0001,10,0.1)\n",
    "# factors = [1,0.75,0.55]\n",
    "\n",
    "# # Tsunami\n",
    "# # Get max mean pga\n",
    "# mu_list = [0.178,0.4255,1.05,1.5]\n",
    "# sigma_list = [1.1462,0.9395,0.7155,0.5288]\n",
    "# factor = 1\n",
    "# IM = np.arange(0.0001,3,0.1)\n",
    "\n",
    "\n",
    "# transform_fragility_funcs(factor,mu_list,sigma_list,ds_list,IM)\n",
    "\n",
    "# pd.DataFrame.from_dict({ds:[lognorm(s=sigma,scale=math.exp(mu*factor)).cdf(im) for im in IM] for mu,sigma,ds in zip(mu_list,sigma_list,ds_list)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_shaking_a = pd.DataFrame.from_dict({\"DS1\":P_ds1a,\"DS2\":P_ds2a,\"DS3\":P_ds3a,\"DS4\":P_ds4a})\n",
    "df_ds_shaking_a.index = IM\n",
    "\n",
    "df_ds_shaking_b = pd.DataFrame.from_dict({\"DS1\":P_ds1b,\"DS2\":P_ds2b,\"DS3\":P_ds3b,\"DS4\":P_ds4b})\n",
    "df_ds_shaking_b.index = IM\n",
    "\n",
    "df_ds_shaking_c = pd.DataFrame.from_dict({\"DS1\":P_ds1c,\"DS2\":P_ds2c,\"DS3\":P_ds3c,\"DS4\":P_ds4c})\n",
    "df_ds_shaking_c.index = IM\n",
    "\n",
    "df_ds_shaking_d = pd.DataFrame.from_dict({\"DS1\":P_ds1d,\"DS2\":P_ds2d,\"DS3\":P_ds3d,\"DS4\":P_ds4d})\n",
    "df_ds_shaking_d.index = IM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(df_ds_shaking):\n",
    "    df_ds_shaking[\"DS3\"] = df_ds_shaking.DS3-df_ds_shaking.DS4\n",
    "    df_ds_shaking[\"DS2\"] = df_ds_shaking.DS2-df_ds_shaking.DS3-df_ds_shaking.DS4\n",
    "    df_ds_shaking[\"DS1\"] = df_ds_shaking.DS1-df_ds_shaking.DS2-df_ds_shaking.DS3-df_ds_shaking.DS4\n",
    "\n",
    "    # Now add zero\n",
    "    ds_shaking_dict = {}\n",
    "\n",
    "    for index,row in df_ds_shaking.iterrows():\n",
    "        row['DS0'] = (1-np.sum(row))\n",
    "        ds_shaking_dict.update({\n",
    "            index:row\n",
    "        })\n",
    "\n",
    "    df_ds_shaking = pd.DataFrame.from_dict(ds_shaking_dict,orient='index')\n",
    "    \n",
    "    return(df_ds_shaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_shaking_a = normalise(df_ds_shaking_a)\n",
    "df_ds_shaking_b = normalise(df_ds_shaking_b)\n",
    "df_ds_shaking_c = normalise(df_ds_shaking_c)\n",
    "df_ds_shaking_d = normalise(df_ds_shaking_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = plt.plot(df_ds_shaking_a.DS0)\n",
    "l1 = plt.plot(df_ds_shaking_a.DS1)\n",
    "l2 = plt.plot(df_ds_shaking_a.DS2)\n",
    "l3 = plt.plot(df_ds_shaking_a.DS3)\n",
    "l4 = plt.plot(df_ds_shaking_a.DS4)\n",
    "\n",
    "plt.xlim([0,5])\n",
    "\n",
    "plt.legend(['DS0','DS1','DS2','DS3','DS4'])\n",
    "plt.title('Shaking')\n",
    "plt.xlabel('PGA')\n",
    "plt.ylabel('Prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_shaking_a = df_ds_shaking_a[['DS0','DS1','DS2','DS3','DS4']]\n",
    "df_ds_shaking_b = df_ds_shaking_b[['DS0','DS1','DS2','DS3','DS4']]\n",
    "df_ds_shaking_c = df_ds_shaking_c[['DS0','DS1','DS2','DS3','DS4']]\n",
    "df_ds_shaking_d = df_ds_shaking_d[['DS0','DS1','DS2','DS3','DS4']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsunami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsunami_bins = {\n",
    "    'zero':0.0001, #0\n",
    "    'low':0.2001, #<0.2\n",
    "    'medium':1.5001, #0.2-1\n",
    "    'high':2.7001 #>1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot2grid((1,1),(0,0))\n",
    "\n",
    "# Get max mean pga\n",
    "mu_ds1 = 0.5#bins['minimum']\n",
    "mu_ds2 = 1#2*earthquake_diff+bins['minimum']\n",
    "mu_ds3 = 2#3*earthquake_diff+bins['minimum']\n",
    "mu_ds4 = 3.5#4*earthquake_diff+bins['minimum']\n",
    "\n",
    "sigma_ds1 = 0.54#0.00000001\n",
    "sigma_ds2 = 0.58#1\n",
    "sigma_ds3 = 0.49#1\n",
    "sigma_ds4 = 0.57#1\n",
    "\n",
    "IM = IM = np.arange(0.0001,3,0.1)#np.arange(bins['minimum'],earthquake_max,(earthquake_max-bins['minimum'])/1000)\n",
    "\n",
    "from scipy.stats import lognorm\n",
    "import math\n",
    "\n",
    "P_ds1 = [lognorm(s=sigma_ds1,scale=math.exp(mu_ds1)).cdf(im) for im in IM]\n",
    "P_ds2 = [lognorm(s=sigma_ds2,scale=math.exp(mu_ds2)).cdf(im) for im in IM]\n",
    "P_ds3 = [lognorm(s=sigma_ds3,scale=math.exp(mu_ds3)).cdf(im) for im in IM]\n",
    "P_ds4 = [lognorm(s=sigma_ds4,scale=math.exp(mu_ds4)).cdf(im) for im in IM]\n",
    "\n",
    "\n",
    "l1 = plt.plot(IM,P_ds1)\n",
    "l2 = plt.plot(IM,P_ds2)\n",
    "l3 = plt.plot(IM,P_ds3)\n",
    "l4 = plt.plot(IM,P_ds4)\n",
    "\n",
    "# ax.plot([bins['low'],bins['low']],[0,1],ls='--',c='k')\n",
    "# ax.plot([bins['medium'],bins['medium']],[0,1],ls='--',c='k')\n",
    "# ax.plot([bins['high'],bins['high']],[0,1],ls='--',c='k')\n",
    "\n",
    "ax.legend(['DS1','DS2','DS3','DS4'])\n",
    "\n",
    "# plt.plot(IM,P_ds0)\n",
    "# plt.plot(IM,P_ds1)\n",
    "# plt.plot(IM,P_ds2)\n",
    "# plt.plot(IM,P_ds3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a rule to stop the crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tsunami = pd.DataFrame.from_dict({\"DS1\":P_ds1,\"DS2\":P_ds2,\"DS3\":P_ds3,\"DS4\":P_ds4})\n",
    "df_ds_tsunami.index = IM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tsunami[\"DS3\"] = df_ds_tsunami.DS3-df_ds_tsunami.DS4\n",
    "df_ds_tsunami[\"DS2\"] = df_ds_tsunami.DS2-df_ds_tsunami.DS3-df_ds_tsunami.DS4\n",
    "df_ds_tsunami[\"DS1\"] = df_ds_tsunami.DS1-df_ds_tsunami.DS2-df_ds_tsunami.DS3-df_ds_tsunami.DS4\n",
    "\n",
    "# Now add zero\n",
    "ds_tsunami_dict = {}\n",
    "\n",
    "for index,row in df_ds_tsunami.iterrows():\n",
    "    row['DS0'] = (1-np.sum(row))\n",
    "    ds_tsunami_dict.update({\n",
    "        index:row\n",
    "    })\n",
    "    \n",
    "df_ds_tsunami = pd.DataFrame.from_dict(ds_tsunami_dict,orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = plt.plot(df_ds_tsunami.DS0)\n",
    "l1 = plt.plot(df_ds_tsunami.DS1)\n",
    "l2 = plt.plot(df_ds_tsunami.DS2)\n",
    "l3 = plt.plot(df_ds_tsunami.DS3)\n",
    "\n",
    "plt.legend(['DS0','DS1','DS2','DS3'])\n",
    "plt.title('Tsunami')\n",
    "plt.xlabel('Height (m)')\n",
    "plt.ylabel('Prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tsunami = df_ds_tsunami[['DS0','DS1','DS2','DS3','DS4']]\n",
    "df_ds_tsunami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liquefaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liquefaction_bins = {\n",
    "    'A':0, #<0.2\n",
    "    'B':1, #0.2-1\n",
    "    'C':2,\n",
    "    'D':3#>1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's binary, so just make up two rows\n",
    "ds_landslide_dict = {\n",
    "    '0':{\n",
    "        'DS0':1.0,\n",
    "        'DS4':0.0\n",
    "    },\n",
    "    '1':{\n",
    "        'DS0':0.0,\n",
    "        'DS4':1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "df_ds_landslide = pd.DataFrame.from_dict(ds_landslide_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_landslide = pd.DataFrame.from_dict(ds_landslide_dict,orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the prior distribution for this hazard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Low, Medium and High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Make up the definition of low, medium and high\n",
    "# len_pga = len(list(df_ds_shaking.index))\n",
    "# pga_low = list(df_ds_shaking.index)[int((len_pga-1)/4)]\n",
    "# pga_medium = list(df_ds_shaking.index)[int(2*(len_pga-1)/4)]\n",
    "# pga_high = list(df_ds_shaking.index)[int(3*(len_pga-1)/4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_bins['medium']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(list(df_ds_shaking_b.index), key=lambda x:abs(x-earthquake_bins['medium']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_low = min(list(df_ds_shaking_a.index), key=lambda x:abs(x-earthquake_bins['low']))\n",
    "earthquake_medium = min(list(df_ds_shaking_a.index), key=lambda x:abs(x-earthquake_bins['medium']))\n",
    "earthquake_high = min(list(df_ds_shaking_a.index), key=lambda x:abs(x-earthquake_bins['high']))\n",
    "\n",
    "liquefaction_0 = liquefaction_bins['A']\n",
    "liquefaction_1 = liquefaction_bins['B']\n",
    "liquefaction_2 = liquefaction_bins['C']\n",
    "liquefaction_3 = liquefaction_bins['D']\n",
    "\n",
    "tsunami_zero = min(list(df_ds_tsunami.index), key=lambda x:abs(x-tsunami_bins['zero']))\n",
    "tsunami_low = min(list(df_ds_tsunami.index), key=lambda x:abs(x-tsunami_bins['low']))\n",
    "tsunami_medium = min(list(df_ds_tsunami.index), key=lambda x:abs(x-tsunami_bins['medium']))\n",
    "tsunami_high = min(list(df_ds_tsunami.index), key=lambda x:abs(x-tsunami_bins['high']))\n",
    "\n",
    "landslide_no = '0'\n",
    "landslide_yes = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Conditional Prob Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.write_file(\"BN_simple.xdsl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nodes = net.get_all_nodes()\n",
    "# # list_of_nodes.pop(net.get_node('Combined'))\n",
    "# # list_of_nodes.pop(net.get_node('ShakingFragility'))\n",
    "# # list_of_nodes.pop(net.get_node('ShakingFragilityA'))\n",
    "# # list_of_nodes.pop(net.get_node('ShakingFragilityB'))\n",
    "# # list_of_nodes.pop(net.get_node('ShakingFragilityC'))\n",
    "[ net.get_node_id(x) for x in list_of_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node_handle in list_of_nodes:\n",
    "#     # Get a single fragility node\n",
    "#     child_node_name = net.get_node_id(node_handle)\n",
    "    \n",
    "#     print(child_node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_node_names = ['LiquefactionFragility','LandslideFragility','TsunamiFragility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child_node_name in child_node_names:\n",
    "    print(child_node_name)\n",
    "    # Get a single fragility node\n",
    "#     child_node_name = net.get_node_id(node_handle)\n",
    "    # Create a new node for the hazard\n",
    "    new_node_name = re.sub('Fragility','',child_node_name)\n",
    "    net.add_node(pysmile.NodeType.CPT,new_node_name)\n",
    "    # Add the outcomes\n",
    "#     if (new_node_name=='ShakingA')|(new_node_name=='ShakingB')|(new_node_name=='ShakingC'):\n",
    "#         outcomes = ['low','medium','high']\n",
    "    if new_node_name=='Landslide':\n",
    "        outcomes = ['no','yes']\n",
    "    elif new_node_name=='Tsunami':\n",
    "        outcomes = ['zero','low','medium','high']\n",
    "    elif new_node_name=='Liquefaction':\n",
    "        outcomes = ['A','B','C','D']\n",
    "    for outcome in outcomes:\n",
    "        net.add_outcome(new_node_name,outcome)\n",
    "    # Delete the premade states\n",
    "    net.delete_outcome(new_node_name,'State0')\n",
    "    net.delete_outcome(new_node_name,'State1')\n",
    "    # Create connection between the hazard and fragility functions\n",
    "    \n",
    "    net.add_arc(new_node_name,child_node_name)\n",
    "    print(new_node_name)\n",
    "    print(child_node_name)\n",
    "    # Set the values of the conditional probability table for the fragility node\n",
    "    if new_node_name=='Landslide':\n",
    "        cpt_value = \\\n",
    "            list(df_ds_landslide[df_ds_landslide.index==landslide_no].T[landslide_no])+\\\n",
    "            list(df_ds_landslide[df_ds_landslide.index==landslide_yes].T[landslide_yes])\n",
    "    elif new_node_name=='Tsunami':\n",
    "        cpt_value = \\\n",
    "            list(df_ds_tsunami[df_ds_tsunami.index==tsunami_zero].T[tsunami_zero])+\\\n",
    "            list(df_ds_tsunami[df_ds_tsunami.index==tsunami_low].T[tsunami_low])+\\\n",
    "            list(df_ds_tsunami[df_ds_tsunami.index==tsunami_medium].T[tsunami_medium])+\\\n",
    "            list(df_ds_tsunami[df_ds_tsunami.index==tsunami_high].T[tsunami_high])\n",
    "    elif new_node_name=='Liquefaction':\n",
    "        cpt_value = [1,0,0,0,1,0,0,0,1]\n",
    "        \n",
    "    net.set_node_definition(child_node_name,cpt_value)\n",
    "    if new_node_name!='Liquefaction':\n",
    "        net.set_node_definition(new_node_name,[1/len(outcomes)]*len(outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([ls for ls in [[2,3,4],[2,3,4],[2,3,4]]],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a single fragility node\n",
    "# child_node_name = net.get_node_id(node_handle)\n",
    "# Create a new node for the hazard\n",
    "new_node_name = 'Shaking'\n",
    "\n",
    "net.add_node(pysmile.NodeType.CPT,new_node_name)\n",
    "\n",
    "outcomes = ['low','medium','high']\n",
    "\n",
    "for outcome in outcomes:\n",
    "    net.add_outcome(new_node_name,outcome)\n",
    "# Delete the premade states\n",
    "net.delete_outcome(new_node_name,'State0')\n",
    "net.delete_outcome(new_node_name,'State1')\n",
    "# Create connection between the hazard and fragility functions\n",
    "\n",
    "for node in ['ShakingFragilityA','ShakingFragilityB','ShakingFragilityC','ShakingFragilityD']:\n",
    "    net.add_arc(new_node_name,node)\n",
    "\n",
    "# Set the values of the conditional probability table for the fragility node\n",
    "cpt_value = \\\n",
    "    list(df_ds_shaking_a[df_ds_shaking_a.index==earthquake_low].T[earthquake_low])+\\\n",
    "    list(df_ds_shaking_a[df_ds_shaking_a.index==earthquake_medium].T[earthquake_medium])+\\\n",
    "    list(df_ds_shaking_a[df_ds_shaking_a.index==earthquake_high].T[earthquake_high])\n",
    "net.set_node_definition('A',cpt_value)\n",
    "\n",
    "cpt_value = \\\n",
    "    list(df_ds_shaking_b[df_ds_shaking_b.index==earthquake_low].T[earthquake_low])+\\\n",
    "    list(df_ds_shaking_b[df_ds_shaking_b.index==earthquake_medium].T[earthquake_medium])+\\\n",
    "    list(df_ds_shaking_b[df_ds_shaking_b.index==earthquake_high].T[earthquake_high])\n",
    "net.set_node_definition('B',cpt_value)\n",
    "\n",
    "cpt_value = \\\n",
    "    list(df_ds_shaking_c[df_ds_shaking_c.index==earthquake_low].T[earthquake_low])+\\\n",
    "    list(df_ds_shaking_c[df_ds_shaking_c.index==earthquake_medium].T[earthquake_medium])+\\\n",
    "    list(df_ds_shaking_c[df_ds_shaking_c.index==earthquake_high].T[earthquake_high])\n",
    "net.set_node_definition('C',cpt_value)\n",
    "\n",
    "cpt_value = \\\n",
    "    list(df_ds_shaking_d[df_ds_shaking_d.index==earthquake_low].T[earthquake_low])+\\\n",
    "    list(df_ds_shaking_d[df_ds_shaking_d.index==earthquake_medium].T[earthquake_medium])+\\\n",
    "    list(df_ds_shaking_d[df_ds_shaking_d.index==earthquake_high].T[earthquake_high])\n",
    "net.set_node_definition('D',cpt_value)\n",
    "\n",
    "\n",
    "net.set_node_definition('Shaking',[1/3]*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add connection between  shaking and liquefaction\n",
    "net.add_arc('Shaking','Liquefaction')\n",
    "net.set_node_definition('Liquefaction',[0.8,0.2,0,0.4,0.5,0.1,0.1,0.3,0.6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_node_position('Shaking',400,50,50,50)\n",
    "net.set_node_position('Liquefaction',700,50,50,50)\n",
    "net.set_node_position('Tsunami',900,50,50,50)\n",
    "net.set_node_position('Landslide',1100,50,50,50)\n",
    "\n",
    "net.set_node_position('ShakingFragilityA',200,175,50,50)\n",
    "net.set_node_position('ShakingFragilityB',400,175,50,50)\n",
    "net.set_node_position('ShakingFragilityC',600,175,50,50)\n",
    "\n",
    "net.set_node_position('ShakingFragility',400,300,50,50)\n",
    "net.set_node_position('LiquefactionFragility',700,300,50,50)\n",
    "net.set_node_position('TsunamiFragility',900,300,50,50)\n",
    "net.set_node_position('LandslideFragility',1100,300,50,50)\n",
    "\n",
    "net.set_node_position('CombinedFragility',800,425,50,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in net.get_all_nodes():\n",
    "    net.set_node_chart_enabled(node,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.write_file(\"test.xdsl\")\n",
    "# frag_model_dict['model'][0].update({\n",
    "#     'net':net\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add hazard evidence to dict\n",
    "hazards_evidence_dict = {\n",
    "    'Landslide':[1,0],\n",
    "    'Tsunami':[0,1,0,0],\n",
    "    'Shaking':[1,0,0]\n",
    "}\n",
    "frag_model_dict = BNModel().add_evidence_to_dict(frag_model_dict,hazards_evidence_dict)\n",
    "\n",
    "# Set evidence and get beliefs\n",
    "frag_model_dict = BNModel().update_evidence(frag_model_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pga = 0.42482\n",
    "sigma_pga = 2.1371\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping over grid, calculating probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each of the hazards, and get the relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hazards = pd.read_csv('example_data.csv')\n",
    "hazard_keys = ['_shaking_prob','_liq','_tsunami','_landslide']\n",
    "hazard_node_names = ['Shaking','Liquefaction','Tsunami','Landslide']\n",
    "\n",
    "for index,row in df_hazards.iterrows():\n",
    "    \n",
    "    # Create evidence dict, one hazard at a time\n",
    "    hazards_evidence_dict = {}\n",
    "    for key,node_name in zip(hazard_keys,hazard_node_names):\n",
    "        hazard_cols = [x for x in df_hazards.columns if key in x]\n",
    "            \n",
    "        hazards_evidence_dict.update({\n",
    "            node_name:list(row[hazard_cols])\n",
    "        })\n",
    "        asdf\n",
    "    # Add hazard evidence to dict\n",
    "    frag_model_dict = BNModel().add_evidence_to_dict(frag_model_dict,hazards_evidence_dict)\n",
    "    \n",
    "    # Set evidence and get beliefs\n",
    "    frag_model_dict = BNModel().update_evidence(frag_model_dict)\n",
    "    \n",
    "    asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_node_value('Combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_node_definition('Combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_outcome_ids('Combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hazard evidence to dict\n",
    "frag_model_dict = BNModel().add_evidence_to_dict(frag_model_dict,hazards_evidence_dict)\n",
    "\n",
    "# Set evidence and get beliefs\n",
    "frag_model_dict = BNModel().update_evidence(frag_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazards_evidence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_all_node_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the cpt for deciding between the PGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Definition of bins for each hazard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:16.558430Z",
     "start_time": "2021-06-10T02:18:09.303508Z"
    }
   },
   "outputs": [],
   "source": [
    "df_hazards = pd.read_csv('example_data.csv')\n",
    "df_hazards_subset = df_hazards.drop(['lon','lat'],axis=1)\n",
    "hazards_data_dict = {col:np.array(content) for col,content in df_hazards_subset.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hazards = df_hazards[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variable Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.234568Z",
     "start_time": "2021-06-10T02:19:00.159611Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Don't include spaces in bin names. if no discretisation, just leave out that key\n",
    "hazards_model_dict = {\n",
    "    'variables':{\n",
    "        'meanpga_lo':{\n",
    "            'label':'meanpga_lo',\n",
    "            'discretisation':{\n",
    "                'n_bins':3,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['Low','Mid','High']\n",
    "            },\n",
    "            'child_nodes':['Liq_Class','land_LP2500_max']\n",
    "        },\n",
    "        'Liq_Class':{\n",
    "            'label':r'Liq_Class',\n",
    "            'child_nodes':[],\n",
    "            'bins':np.unique(df_hazards['Liq_Class'])\n",
    "        },\n",
    "        'land_LP2500_max':{\n",
    "            'label':'land_LP2500_max',\n",
    "            'discretisation':{\n",
    "                'n_bins':3,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['Low','Mid','High']\n",
    "            },\n",
    "            'child_nodes':[]\n",
    "        }\n",
    "    },\n",
    "    'training_frac':0.8,\n",
    "    'bootstrap_reps':1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.754301Z",
     "start_time": "2021-06-10T02:19:00.326513Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bootstrap the data, and add it to the model_dict\n",
    "hazards_model_dict = BNModel().bootstrap_data(hazards_model_dict,hazards_data_dict,df_hazards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazards_model_dict['variables'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:46.021235Z",
     "start_time": "2021-06-10T02:19:00.755300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discretise the data\n",
    "hazards_file_label = \"hazards\"\n",
    "\n",
    "hazards_model_dict = BNModel().discretiser(hazards_model_dict,[\"Liq_Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.387291Z",
     "start_time": "2021-06-10T02:19:46.102171Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hazards_disc_fig = BNModel().plot_discretiser(hazards_model_dict,[\"Liq_Class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.711518Z",
     "start_time": "2021-06-10T02:20:53.985216Z"
    }
   },
   "outputs": [],
   "source": [
    "BNModel().save_dataset(hazards_model_dict,hazards_file_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:21:05.864450Z",
     "start_time": "2021-06-10T02:21:05.152790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the BN\n",
    "hazards_model_dict = BNModel().create_SM(hazards_model_dict,hazards_file_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Conditional Probability tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.809434Z",
     "start_time": "2021-06-10T02:20:56.573Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get conditional probs tables\n",
    "BNModel().get_conditional_prob_table(hazards_model_dict,'land_LP2500_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.814431Z",
     "start_time": "2021-06-10T02:20:56.725Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Get conditional probs tables\n",
    "# df_CPT_MSL = BNModel().get_conditional_prob_table(ocean_model_dict,'TWL')\n",
    "\n",
    "# df_CPT_MSL = df_CPT_MSL.loc[['VeryLow','Low','Mid','High','VeryHigh']]\n",
    "\n",
    "# plt.pcolor(df_CPT_MSL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.833418Z",
     "start_time": "2021-06-10T02:20:57.381Z"
    }
   },
   "outputs": [],
   "source": [
    "hazards_evidence_dict = {\n",
    "    'land_LP2500_max':[0.05,0.1,0.01]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.838415Z",
     "start_time": "2021-06-10T02:20:57.549Z"
    }
   },
   "outputs": [],
   "source": [
    "hazards_model_dict = BNModel().add_evidence_to_dict(hazards_model_dict,hazards_evidence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update beliefs based on evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.843410Z",
     "start_time": "2021-06-10T02:20:57.946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set evidence and get beliefs\n",
    "hazards_model_dict = BNModel().update_evidence(hazards_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.849407Z",
     "start_time": "2021-06-10T02:20:58.285Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up graph\n",
    "graph_hazards = BNModel().create_BN_graph()\n",
    "\n",
    "# Create nodes of the graph\n",
    "graph_hazards,hazards_model_dict = BNModel().create_nodes(graph_hazards,hazards_model_dict,0)\n",
    "\n",
    "# Create arcs between nodes\n",
    "graph_hazards = BNModel().create_arcs(graph_hazards,hazards_model_dict)\n",
    "\n",
    "# Save as dot file\n",
    "graph_hazards.render(filename='graph_hazards',format='png')\n",
    "\n",
    "# Plot the graph\n",
    "graph_hazards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "2344856/I2LE4LVY": {
     "DOI": "10.1029/2019PA003589",
     "author": [
      {
       "family": "Bengtson",
       "given": "Shannon A."
      },
      {
       "family": "Meissner",
       "given": "Katrin J."
      },
      {
       "family": "Menviel",
       "given": "Laurie"
      },
      {
       "family": "A. Sisson",
       "given": "Scott"
      },
      {
       "family": "Wilkin",
       "given": "John"
      }
     ],
     "container-title": "Paleoceanography and Paleoclimatology",
     "container-title-short": "Paleoceanography and Paleoclimatology",
     "id": "2344856/I2LE4LVY",
     "issued": {
      "day": 17,
      "month": 5,
      "year": 2019
     },
     "journalAbbreviation": "Paleoceanography and Paleoclimatology",
     "note": "Citation Key: bengtson2019evaluating",
     "page": "1022-1036",
     "page-first": "1022",
     "title": "Evaluating the extent of North Atlantic Deep Water and the mean Atlantic <sup>13</sup>C from statistical reconstructions",
     "type": "article-journal",
     "volume": "34"
    },
    "2344856/V5HIVSEQ": {
     "DOI": "10.1017/S0263593300020782",
     "URL": "https://www.cambridge.org/core/journals/earth-and-environmental-science-transactions-of-royal-society-of-edinburgh/article/an-alternative-astronomical-calibration-of-the-lower-pleistocene-timescale-based-on-odp-site-677/D02E93BFBF418256AD00642C8A98277C",
     "abstract": "Ocean Drilling Program (ODP) Site 677 provided excellent material for high resolution stable isotope analysis of both benthonic and planktonic foraminifera through the entire Pleistocene and upper Pliocene. The oxygen isotope record is readily correlated with the SPECMAP stack (Imbrie et al. 1984) and with the record from DSDP 607 (Ruddiman et al. 1986) but a significantly better match with orbital models is obtained by departing from the timescale proposed by these authors below Stage 16 (620 000 years). It is the stronger contribution from the precession signal in the record from ODP Site 677 that provides the basis for the revised timescale. Our proposed modification to the timescale would imply that the currently adopted radiometric dates for the MatuyamaBrunhes boundary, the Jaramillo and Olduvai Subchrons and the GaussMatuyama boundary underestimate their true astronomical ages by between 5 and 7%.",
     "accessed": {
      "day": 19,
      "month": 5,
      "year": 2020
     },
     "author": [
      {
       "family": "Shackleton",
       "given": "N. J."
      },
      {
       "family": "Berger",
       "given": "A."
      },
      {
       "family": "Peltier",
       "given": "W. R."
      }
     ],
     "container-title": "Earth and Environmental Science Transactions of The Royal Society of Edinburgh",
     "id": "2344856/V5HIVSEQ",
     "issue": "4",
     "issued": {
      "year": 1990
     },
     "language": "en",
     "note": "citation key: shackleton1990alternative",
     "page": "251-261",
     "page-first": "251",
     "title": "An alternative astronomical calibration of the lower Pleistocene timescale based on ODP Site 677",
     "type": "article-journal",
     "volume": "81"
    }
   }
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "744px",
    "left": "1262px",
    "right": "20px",
    "top": "135px",
    "width": "279px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
